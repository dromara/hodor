jobtype.class=org.dromara.hodor.actuator.bigdata.jobtype.asyncSpark.AsyncSparkJob

# 版本设置 ${hdp.version}替换
spark.executor.extraJavaOptions=-Dhdp.version=2.6.5.0-292
spark.driver.extraJavaOptions=-Dhdp.version=2.6.5.0-292
spark.am.extraJavaOptions=-Dhdp.version=2.6.5.0-292

# spark task history
spark.eventLog.enabled=true
spark.yarn.historyServer.address=http://10.4.6.108:18081
spark.eventLog.dir=hdfs:///spark2-history/
spark.history.fs.logDirectory=hdfs:///spark2-history/

# yarn config
yarn.jars=hdfs://bigdata-hdp-nodename-2/tmp/spark23/*.jar
yarn.resource.manager.address=bigdata-hdp-nodename-1:8032,bigdata-hdp-nodename-2:8032

# hdfs HA config
# 配置为fs.defalutFS的值
hdfs.defalutFS=hdfs://testcluster
# dfs.nameservices的配置
hdfs.nameservices=testcluster
# dfs.ha.namenodes的配置
hdfs.namenodes=nn1,nn2
# dfs.namenode.rpc-address的配置，注意rpc-address的配置要和namenodes匹配，不然会配置失效
hdfs.rpc-address=bigdata-hdp-nodename-1:8020,bigdata-hdp-nodename-2:8020